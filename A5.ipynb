{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "596-Assignment-5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FuO82RTBftK"
      },
      "source": [
        "# Language Modeling\n",
        "\n",
        "In this assignment, let's generate text using a trigram language model.\n",
        "\n",
        "Go to https://drive.google.com/drive/folders/1Pe6D713L9S0GWwb_XzeLXAeNZOrBqZaN?usp=sharing and click add shortcut to drive. This will add the data required for this problem set to your Google drive.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LqHisiziX8Ri94Xs6Cv8mhx6vivFM3kS\" alt=\"Drawing\" height=\"300\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZEcHthBeXz"
      },
      "source": [
        "Run the below code snippet. It will generate a URL which generates an authorization code.* Enter it below to give Colab access to your Google drive. \n",
        "\n",
        "*Copy function may not work. If so, manually copy the authorization code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW-dce7oJlyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02f2793-545b-4f76-f8a7-7290fc50f433"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni2pYuuQKaHY"
      },
      "source": [
        "When you run the `ls` command below, you should see the files in the tweets folder.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYENtyc7SOxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf9cf1a6-b22d-4816-9968-c0436b12de38"
      },
      "source": [
        "!ls \"/content/drive/My Drive/tweets\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000_tweets.jsonl\n",
            "20000_tweets.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.tokenized.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.trigrams.txt\n",
            "GoogleNews-vectors-negative300.bin.gz\n",
            "stop_words.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2Y7I_9lPoZS"
      },
      "source": [
        "Let's load the trigrams first. You can change the below code as you see fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZMOmElPSPHk"
      },
      "source": [
        "from math import log\n",
        "\n",
        "bigram_prefix_to_trigram = {}\n",
        "bigram_prefix_to_trigram_weights = {}\n",
        "\n",
        "lines = open(\"/content/drive/My Drive/tweets/covid-tweets-2020-08-10-2020-08-21.trigrams.txt\").readlines()\n",
        "for line in lines:\n",
        "  word1, word2, word3, count = line.strip().split()\n",
        "\n",
        "  #initiating new entries if the bigram was not used yet\n",
        "  if (word1, word2) not in bigram_prefix_to_trigram:\n",
        "    bigram_prefix_to_trigram[(word1, word2)] = []\n",
        "    bigram_prefix_to_trigram_weights[(word1, word2)] = []\n",
        "\n",
        "  #two dictionaries, have bigram as key and array of 3rd words or counts as value\n",
        "  bigram_prefix_to_trigram[(word1, word2)].append(word3)\n",
        "  bigram_prefix_to_trigram_weights[(word1, word2)].append(int(count))\n",
        "\n",
        "\n",
        "# freeup memory\n",
        "lines = None"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X48i3rarPzd8"
      },
      "source": [
        "\n",
        "## Problem 1: Retrieve top next words and their probability given a bigram prefix.\n",
        "\n",
        "For the following prefixes **word1=middle, word2=of, and n=10**, the output is:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "a 0.807981220657277\n",
        "the 0.06948356807511737\n",
        "pandemic 0.023943661971830985\n",
        "this 0.016901408450704224\n",
        "an 0.0107981220657277\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYhal88xSYow",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7e0c5d-106f-4e87-d25f-92511fcd8019"
      },
      "source": [
        "def top_next_word(word1, word2, n):\n",
        "\n",
        "  #add up the counts of all third words that follow the bigram\n",
        "  total_count=0\n",
        "  for i in range(len(bigram_prefix_to_trigram_weights[(word1, word2)])):\n",
        "    total_count += bigram_prefix_to_trigram_weights[(word1, word2)][i]\n",
        "\n",
        "  #create empty lists that will be returned\n",
        "  next_words=[]\n",
        "  probs=[]\n",
        "\n",
        "  #iterate through n top elements of the bigram's arrays\n",
        "  for i in range(n):\n",
        "    #store the third word\n",
        "    next_words.append(bigram_prefix_to_trigram[(word1, word2)][i])\n",
        "    #store the probability as the word's count / total count\n",
        "    probs.append(bigram_prefix_to_trigram_weights[(word1, word2)][i]/total_count)  \n",
        "\n",
        "  return next_words, probs\n",
        "\n",
        "next_words, probs = top_next_word(\"middle\", \"of\", 10)\n",
        "for word, prob in zip(next_words, probs):\n",
        "  print(word, prob)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a 0.807981220657277\n",
            "the 0.06948356807511737\n",
            "pandemic 0.023943661971830985\n",
            "this 0.016901408450704224\n",
            "an 0.0107981220657277\n",
            "covid 0.009389671361502348\n",
            "nowhere 0.008450704225352112\n",
            "it 0.004694835680751174\n",
            "lockdown 0.002347417840375587\n",
            "summer 0.002347417840375587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gok10i2dSHXB"
      },
      "source": [
        "## Problem 2: Sampling n words\n",
        "\n",
        "Sample next n words given a bigram prefix. Use the probablity distribution defined by the frequency counts. Functions like **numpy.random.choice** will be useful here. Sample without repitition, otherwise all your samples will contain the most frequent trigram.\n",
        "\n",
        "\n",
        "For the following prefixes **word1=middle, word2=of, and n=10**, the output could be as follows (our outputs may differ): \n",
        "\n",
        "```\n",
        "a 0.807981220657277\n",
        "pandemic 0.023943661971830985\n",
        "nowhere 0.008450704225352112\n",
        "the 0.06948356807511737\n",
        "...\n",
        "...\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OzYJoYfUaom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa5a835-acc4-4a1e-8c32-f1527cb942ea"
      },
      "source": [
        "import numpy\n",
        "\n",
        "def sample_next_word(word1, word2, n):\n",
        "\n",
        "  #add up the counts of all third words that follow the bigram\n",
        "  total_count=0\n",
        "  for i in range(len(bigram_prefix_to_trigram_weights[(word1, word2)])):\n",
        "    total_count += bigram_prefix_to_trigram_weights[(word1, word2)][i]\n",
        "\n",
        "  #create a probability distribution list\n",
        "  prob_distribution=[]\n",
        "  for i in range(len(bigram_prefix_to_trigram_weights[(word1, word2)])):\n",
        "    prob_distribution.append(bigram_prefix_to_trigram_weights[(word1, word2)][i]/total_count)  \n",
        "\n",
        "  #generate list of indexes for the sample\n",
        "  #choosing n indexes within the size of the array, without replacement, using probability distribution for picking\n",
        "  if (n <= len(bigram_prefix_to_trigram_weights[(word1, word2)])):\n",
        "    list_of_indexes = numpy.random.choice(len(bigram_prefix_to_trigram_weights[(word1, word2)]), n, replace=False, p=prob_distribution)\n",
        "  else:\n",
        "    list_of_indexes = numpy.random.choice(len(bigram_prefix_to_trigram_weights[(word1, word2)]), len(bigram_prefix_to_trigram_weights[(word1, word2)]), replace=False, p=prob_distribution)\n",
        "\n",
        "  #create empty lists that will be returned\n",
        "  next_words=[]\n",
        "  probs=[]\n",
        "\n",
        "  #iterate through the entries based on the created list of indexes\n",
        "  for i in list_of_indexes:\n",
        "    #store the third word\n",
        "    next_words.append(bigram_prefix_to_trigram[(word1, word2)][i])\n",
        "    #store the probability as the word's count / total count\n",
        "    probs.append(bigram_prefix_to_trigram_weights[(word1, word2)][i]/total_count)  \n",
        "\n",
        "  return next_words, probs\n",
        "  # write your code here\n",
        "\n",
        "next_words, probs = sample_next_word(\"middle\", \"of\", 10)\n",
        "for word, prob in zip(next_words, probs):\n",
        "  print(word, prob)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a 0.807981220657277\n",
            "pandemic 0.023943661971830985\n",
            "the 0.06948356807511737\n",
            "endorsement 0.00046948356807511736\n",
            "this 0.016901408450704224\n",
            "an 0.0107981220657277\n",
            "#covid19 0.0018779342723004694\n",
            "no 0.0009389671361502347\n",
            "that 0.00046948356807511736\n",
            "summer 0.002347417840375587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyYenU8H-fIR"
      },
      "source": [
        "## Problem 3: Generate sentences starting with a prefix\n",
        "\n",
        "Generates n-sentences starting with a given sentence prefix. Use [beam search](https://en.wikipedia.org/wiki/Beam_search) to generate multiple sentences. Depending on which method you use to generate next word, you will get different outputs. When you generate <EOS> in a path, stop exploring that path.\n",
        "\n",
        "If you use the method `word_generator=top_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> trump`, your output is as follows:\n",
        "```\n",
        "<BOS1> <BOS2> trump eyes new unproven coronavirus treatment URL <EOS> 0.00021893147502903603\n",
        "<BOS1> <BOS2> trump eyes new unproven coronavirus cure URL <EOS> 0.0001719607222046247\n",
        "<BOS1> <BOS2> trump eyes new unproven virus cure promoted by mypillow ceo over unproven therapeutic URL <EOS> 9.773272077557522e-05\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n",
        "\n",
        "\n",
        "If you use the method `word_generator=top_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> biden`, your output is as follows:\n",
        "```\n",
        "<BOS1> <BOS2> biden calls for a 30 bonus URL #cashgem #cashappfriday #stayathome <EOS> 0.0002495268686322749\n",
        "<BOS1> <BOS2> biden says all u.s. governors should mandate masks <EOS> 1.6894510541025754e-05\n",
        "<BOS1> <BOS2> biden says all u.s. governors question cost of a pandemic <EOS> 8.777606198953028e-07\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n",
        "\n",
        "\n",
        "If you use the method `word_generator=sample_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> trump`, your output may look as follows (since this is sampling, our outputs will difer):\n",
        "\n",
        "```\n",
        "<BOS1> <BOS2> trump signs executive orders URL <EOS> 7.150992253427233e-05\n",
        "<BOS1> <BOS2> trump signs executive actions URL <EOS> 7.117242889600614e-05\n",
        "<BOS1> <BOS2> trump news president attacked over it <EOS> 1.0546494007903964e-05\n",
        "<BOS1> <BOS2> trump news president attacked over executive orders URL <EOS> 1.0126405114118984e-05\n",
        "```\n",
        "\n",
        "If you use the method `word_generator=sample_next_word`, `beam=10` and prefix is `<BOS1> <BOS2> biden`, your output may look as follows:\n",
        "\n",
        "```\n",
        "<BOS1> <BOS2> biden harris 2020 <EOS> 0.0015758924114719264\n",
        "<BOS1> <BOS2> biden harris 2020 URL <EOS> 0.0006443960952032196\n",
        "<BOS1> <BOS2> biden calls for evictions ban so marylander 's do it URL <EOS> 4.105215709355001e-07\n",
        "<BOS1> <BOS2> biden calls for evictions ban so marylander 's do our best to stay home <EOS> 1.3158806336098573e-09\n",
        "...\n",
        "...\n",
        "...\n",
        "...\n",
        "...\n",
        "```\n",
        "\n",
        "Hope you see that sampling gives different outputs compared to deterministically picking the top n-words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoBYgm4ZNNz3",
        "outputId": "ba5bb3c7-8b65-4742-ad52-fd6349a38f3d"
      },
      "source": [
        "sentence =\"Hello <EOD>\"\n",
        "sentence_words=sentence.split()\n",
        "word1=sentence_words[len(sentence_words)-2]\n",
        "word2=sentence_words[len(sentence_words)-1]\n",
        "print(word1, word2)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello <EOD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40kW0joweXFO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "bc43b543-fa03-4e23-9a3c-ffde26f09a26"
      },
      "source": [
        "def generate_sentences(prefix, beam, sampler):\n",
        "  word0, word1, word2 = prefix.strip().split()\n",
        "\n",
        "  top_sentences=[prefix]\n",
        "  top_sentence_probs=[1]\n",
        "\n",
        "\n",
        "  remains = True\n",
        "\n",
        "  while(remains==True):\n",
        "    remains=False\n",
        "\n",
        "    for sentence_i in range(len(top_sentences)):  \n",
        "\n",
        "      top_sentences[sentence_i] = top_sentences[sentence_i].split()\n",
        "      word1=top_sentences[sentence_i][len(top_sentences[sentence_i])-2]\n",
        "      word2=top_sentences[sentence_i][len(top_sentences[sentence_i])-1]\n",
        "      print(top_sentences[sentence_i])\n",
        "      print(type(word1), type(word2), word1, word2)\n",
        "    \n",
        "\n",
        "      if word2==\"<EOS>\":\n",
        "        break\n",
        "      candidate_sentences=top_sentences[sentence_i]\n",
        "      candidate_sentence_probs=[]\n",
        "\n",
        "      #generate candidate words\n",
        "      if sampler==top_next_word:\n",
        "        next_words, probs = top_next_word(str(word1), str(word2), beam)\n",
        "      if sampler==sample_next_word:\n",
        "        next_words, probs = sample_next_word(word1, word2, beam)\n",
        "\n",
        "      #create candidate sentences and calculate their probabilities\n",
        "      for i in range(len(next_words)):\n",
        "        candidate_sentences.append(next_words[i])\n",
        "        candidate_sentence_probs.append(top_sentence_probs[sentence_i]*probs[i])\n",
        "        if next_words[i]!=\"<EOS>\":\n",
        "          remains=True\n",
        "        \n",
        "      #from all candidate sentences created from top sentences, would choose the new top sentences...\n",
        "      top_sentences = candidate_sentences\n",
        "      top_sentence_probs = candidate_sentence_probs\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Candidate sentences: \", candidate_sentences)\n",
        "  print(\"Candidate sentence probs: \", candidate_sentence_probs)\n",
        "\n",
        "\n",
        "  sentences=[ ]\n",
        "  probs=[ ]\n",
        "  return sentences, probs \n",
        "  # write your code\n",
        "\n",
        "\n",
        "sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> trump\", beam=2, sampler=top_next_word)\n",
        "for sent, prob in zip(sentences, probs):\n",
        "  print(sent, prob)\n",
        "print(\"#########################\\n\")\n",
        "\n",
        "sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> biden\", beam=2, sampler=top_next_word)\n",
        "for sent, prob in zip(sentences, probs):\n",
        "  print(sent, prob)\n",
        "print(\"#########################\\n\")\n",
        "\n",
        "sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> trump\", beam=2, sampler=sample_next_word)\n",
        "for sent, prob in zip(sentences, probs):\n",
        "  print(sent, prob)\n",
        "print(\"#########################\\n\")\n",
        "\n",
        "sentences, probs = generate_sentences(prefix=\"<BOS1> <BOS2> biden\", beam=2, sampler=sample_next_word)\n",
        "for sent, prob in zip(sentences, probs):\n",
        "  print(sent, prob)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<BOS1>', '<BOS2>', 'trump']\n",
            "<class 'str'> <class 'str'> <BOS2> trump\n",
            "['<BOS1>']\n",
            "<class 'str'> <class 'str'> <BOS1> <BOS1>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-157-2c52386a4a0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<BOS1> <BOS2> trump\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_next_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-157-2c52386a4a0b>\u001b[0m in \u001b[0;36mgenerate_sentences\u001b[0;34m(prefix, beam, sampler)\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;31m#generate candidate words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtop_next_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mnext_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_next_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0msample_next_word\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnext_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_next_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-1f363d522301>\u001b[0m in \u001b[0;36mtop_next_word\u001b[0;34m(word1, word2, n)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#add up the counts of all third words that follow the bigram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mtotal_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_prefix_to_trigram_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbigram_prefix_to_trigram_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('<BOS1>', '<BOS1>')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCpo9Q-nf4N8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q-FpH2k7akG"
      },
      "source": [
        "# Semantic Parsing\n",
        "\n",
        "We will use Wikidata and SPARQL to do semantic parsing, i.e., convert a question to a SPARQL query and execute it on Wikidata to get the answer. First install sparqlwrapper which allows us to write and execute sparql queries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGbvLdTPQgPX",
        "outputId": "b2da0a38-1bac-4059-a511-62e11bba51fd"
      },
      "source": [
        "!pip install sparqlwrapper"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sparqlwrapper\n",
            "  Downloading https://files.pythonhosted.org/packages/00/9b/443fbe06996c080ee9c1f01b04e2f683b2b07e149905f33a2397ee3b80a2/SPARQLWrapper-1.8.5-py3-none-any.whl\n",
            "Collecting rdflib>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/6b/6454aa1db753c0f8bc265a5bd5c10b5721a4bb24160fb4faf758cf6be8a1/rdflib-5.0.0-py3-none-any.whl (231kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.0->sparqlwrapper) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib>=4.0->sparqlwrapper) (1.15.0)\n",
            "Collecting isodate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n",
            "\u001b[?25hInstalling collected packages: isodate, rdflib, sparqlwrapper\n",
            "Successfully installed isodate-0.6.0 rdflib-5.0.0 sparqlwrapper-1.8.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDFiQz3wS1cz"
      },
      "source": [
        "Here is an example SPARQL query for \"countries and their population sorted in descending order\". You can play with several example queries here https://query.wikidata.org/ (many examples are also given in this page)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Jcaf1T9mHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea096be3-78e9-4695-dcc9-c41bb36ed590"
      },
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "sparql.setQuery(\"\"\"\n",
        "SELECT DISTINCT ?countryLabel ?population\n",
        "{\n",
        "  ?country wdt:P31 wd:Q6256 ;\n",
        "           wdt:P1082 ?population .\n",
        "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
        "}\n",
        "GROUP BY ?population ?countryLabel\n",
        "ORDER BY DESC(?population)\n",
        "\"\"\")\n",
        "sparql.setReturnFormat(JSON)\n",
        "results = sparql.query().convert()\n",
        "\n",
        "for result in results[\"results\"][\"bindings\"][:10]:\n",
        "    print('%s\\t%s' % (result[\"countryLabel\"][\"value\"], result[\"population\"][\"value\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "People's Republic of China\t1409517397\n",
            "India\t1326093247\n",
            "United States of America\t328239523\n",
            "Indonesia\t263991379\n",
            "Pakistan\t216565318\n",
            "Brazil\t210147125\n",
            "Nigeria\t190886311\n",
            "Bangladesh\t164669751\n",
            "Mongol Empire\t160000000\n",
            "Mexico\t130526945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0vsfuFnT-9W"
      },
      "source": [
        "The main challenge here is to identify what properties and entities you are interested in. You can use [Wikidata search box](https://www.wikidata.org/wiki/Wikidata:Main_Page) to find an entity id and then finding relevant property ids."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXHjFq6upAUA"
      },
      "source": [
        "\n",
        "## Problem 4: What are the cities in Quebec?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvff7yDBFUMg"
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqr0abgqUU0Z"
      },
      "source": [
        "## Problem 5: What is the most populated city in Quebec and its population?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rtH3-87UU0Z"
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBW50NXCWKDF"
      },
      "source": [
        "## Problem 6: Who are the current premiers of different Canadian provinces sorted by their population?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3teIPTlWKDG"
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naf-3cXzgP4M"
      },
      "source": [
        "## Problem 7: What is the province in Canada that borders the most number of provinces?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPyARfPugP4M"
      },
      "source": [
        "# Write your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}